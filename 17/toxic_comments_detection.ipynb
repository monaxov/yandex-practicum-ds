{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a0f38a",
   "metadata": {},
   "source": [
    "# Опыт классификации комментариев товаров интернет-магазина с помощью классических моделей машинного обучения.\n",
    "Целью проекта являлся подбор модели машинного обучения, которая бы разделяла комментарии от покупателей интернет-магазина на токсичные и нейтральные с качеством работы не менее 0.75 по метрике f1.\n",
    "\n",
    "В ходе работы над проектом имеющиеся данные были изучены и предобработаны определенным образом с целью получить максимальный уровень качества  на одной из классических моделей машинного обучения и векторизации на основе метрики TF-IDF.\n",
    "Исследование проводилось на основе  набора из 159292  размеченных текстов сообщений на английском языке.\n",
    "\n",
    "В ходе изучения данных было выяснено, что в половине всех случаев токсичность сочетается с наличием обсценной лексики, на основе этого наблюдения было решено выделить ее из сообщений, что преследовало двойную цель:\n",
    "- демаскировать скрытую грубость, когда оскорбления встраиваются в текст без пробелов\n",
    "- усилить вес подобных слов\n",
    "\n",
    "В исследование вошли следующие этапы:\n",
    "- Проверка данных на готовность к обработке\n",
    "- Выделение обсценной лексики из сообщений в отдельный признак\n",
    "- Обработка основного текстового признака\n",
    "    - токенизация с фильтрация неалфавитных символов\n",
    "    - фильтрация по длине и количеству слов\n",
    "    - удаление стоп-слов\n",
    "    - стеммизация\n",
    "- формирование нового признака из обработанного текстового признака и признака обсценных слов\n",
    "- Создание конвейера автоматизированной обработки с\n",
    "    - векторизатором на основе метрики tf-idf\n",
    "    - ridge-классификатором\n",
    "- кросс-валидация и подбор гиперпараметров векторизатора и классификатора вручную\n",
    "- Замена ridge-классификатора на  несколько других вариантов поочередно\n",
    "- Формулирование  выводов\n",
    "- Финальная проверка на тестовой выборке\n",
    "\n",
    "По результатам проведенных мероприятий была предложена схема на основе tf-idf-векторизатора  и ridge-классификатора, как наиболее оптимальная по временным и вычислительным затратам, при этом дающая необходимый уровень качества\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "323e9c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\monah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\monah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9090a91",
   "metadata": {},
   "source": [
    "<img src=\"https://emojigraph.org/media/apple/check-mark-button_2705.png\" align=left width=33, heigth=33>\n",
    "<div class=\"alert alert-success\">\n",
    "Отлично, все нужные библиотеки импортированы в начале ноутбука.Это хорошая практика.\n",
    "\n",
    "Также вижу, что ты \"расчехлил\" pandarell для ускорения обработки ))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f927e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aacd1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(text, pattern):\n",
    "    \"\"\"Returns a string containing all words in the pattern\"\"\"\n",
    "    import re\n",
    "    return ' '.join(re.findall(pattern, text.lower()))\n",
    "\n",
    "\n",
    "def get_max_length(text):\n",
    "    \"\"\"Returns the longest word in the text\"\"\"\n",
    "    import numpy as np\n",
    "    if len(text) == 0: return ''\n",
    "    words = text.split()\n",
    "    lengths = [len(w) for w in words]\n",
    "    return words[np.argmax(lengths)]\n",
    "\n",
    "def preprocess(text, max_word_length=32, max_words_number=500):\n",
    "    import re\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    lst = re.sub(r'[^A-Za-z]', ' ', text).split()[:max_words_number]\n",
    "    stems = [ps.stem(w) for w in lst if len(w) <= max_word_length and w not in stop_words]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8515c3a8",
   "metadata": {},
   "source": [
    "## Загрузка и изучение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ccaf230",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/datasets/toxic_comments.csv'):\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv', index_col=0)\n",
    "elif os.path.exists('toxic_comments.csv'):\n",
    "    data = pd.read_csv('toxic_comments.csv', index_col=0)\n",
    "else:\n",
    "    print('download failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22aa159a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfeb2d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92964</th>\n",
       "      <td>Dude, get a fucking life you queer.. get a real paying job instead of e-wiki policing around, you 50 year old piece of shite... and your mom said to make your bed.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120398</th>\n",
       "      <td>The only nonsense I will approve is nonsense committed by 1)chaka zulu warriors. 2)anglican gay priests 3)cocksuck ers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47519</th>\n",
       "      <td>Thou shalt be ravished in thy buttocks \\n\\nIt's sure to be hurtin!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>Groin \\n\\nYou sad fuck. I'm going to restore my edit, and you are going to leave it, or else I wil knee you in the groin yourself, prick. 91.108.208.101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55660</th>\n",
       "      <td>Cause of Death \\n\\nFuck exicution, put down Murder was the mutha fuckin' case, and Shawtnegger was tha murdera!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                       text  \\\n",
       "92964   Dude, get a fucking life you queer.. get a real paying job instead of e-wiki policing around, you 50 year old piece of shite... and your mom said to make your bed.   \n",
       "120398                                               The only nonsense I will approve is nonsense committed by 1)chaka zulu warriors. 2)anglican gay priests 3)cocksuck ers   \n",
       "47519                                                                                                    Thou shalt be ravished in thy buttocks \\n\\nIt's sure to be hurtin!   \n",
       "815                Groin \\n\\nYou sad fuck. I'm going to restore my edit, and you are going to leave it, or else I wil knee you in the groin yourself, prick. 91.108.208.101   \n",
       "55660                                                       Cause of Death \\n\\nFuck exicution, put down Murder was the mutha fuckin' case, and Shawtnegger was tha murdera!   \n",
       "\n",
       "        toxic  \n",
       "92964       1  \n",
       "120398      1  \n",
       "47519       1  \n",
       "815         1  \n",
       "55660       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145956</th>\n",
       "      <td>Press \\n June 2011: Today Show, NBC: Today's Money: Money-Maximizing Web Sites\\n June 2011: MORE Magazine: Make Even More Money\\n June 2011: Mint.com: Free Credit Scores (Seriously, They’re Really Free)\\n March 2011: The Wall Street Journal: The Daily Start-up\\n December 2010: Money Crashers: Credit Sesame Review – Free Credit and Debt Management Tool\\n October 2010: Consumer Reports: Finovate Fall Day 1: Financial voyeurism and a free credit score \\n September 2010: CNN Money: Today in Tech: News around the Web</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41086</th>\n",
       "      <td>Contested deletion \\n\\nThis article should not be speedy deleted as lacking sufficient context to identify its subject, because... there is an article on Wikipedia:\\n\\nhttp://nl.wikipedia.org/wiki/Dru_Yoga</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127657</th>\n",
       "      <td>\"\\n  Your latest edits have goen even further towards a Christian POV of the article. For an article on the historicity of Jesus, see that article. This article is not about historicity at all, but you've just gone and made edits that make it seem like it is. And plus, you've dramatically changed what the sources are saying. \\nYou see, what you guys don't seem to understand is that \\n\\n\"\"The term Historical Jesus refers to scholarly reconstructions of the life of Jesus of Nazareth,[3][4][5\"\"\\n\\nIs not the same as \\n\\n\"\"The term Historical Jesus refers to scholarly reconstructions of portraits of the life of Jesus of Nazareth.[3][4][5]\"\"\\n\\nTo go out and dramatically change what the sources are saying is probably a gross misrepresentation of those sources.   \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5828</th>\n",
       "      <td>\"\\nIt should not be deleted, but fixed. North Kosovo is de facto independent from the Albanian-dominated government in Pristina, while other enclaves south are not. speaks \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87827</th>\n",
       "      <td>I'm back... \\n\\n...I haven't found the rusty knife yet, but I'm working on it. Now, I believe we have matters to discuss. Like wtf do you think you are playing at?!?! 217.41.238.156</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text  \\\n",
       "145956                                                                                                                                                                                                                                                              Press \\n June 2011: Today Show, NBC: Today's Money: Money-Maximizing Web Sites\\n June 2011: MORE Magazine: Make Even More Money\\n June 2011: Mint.com: Free Credit Scores (Seriously, They’re Really Free)\\n March 2011: The Wall Street Journal: The Daily Start-up\\n December 2010: Money Crashers: Credit Sesame Review – Free Credit and Debt Management Tool\\n October 2010: Consumer Reports: Finovate Fall Day 1: Financial voyeurism and a free credit score \\n September 2010: CNN Money: Today in Tech: News around the Web   \n",
       "41086                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Contested deletion \\n\\nThis article should not be speedy deleted as lacking sufficient context to identify its subject, because... there is an article on Wikipedia:\\n\\nhttp://nl.wikipedia.org/wiki/Dru_Yoga   \n",
       "127657  \"\\n  Your latest edits have goen even further towards a Christian POV of the article. For an article on the historicity of Jesus, see that article. This article is not about historicity at all, but you've just gone and made edits that make it seem like it is. And plus, you've dramatically changed what the sources are saying. \\nYou see, what you guys don't seem to understand is that \\n\\n\"\"The term Historical Jesus refers to scholarly reconstructions of the life of Jesus of Nazareth,[3][4][5\"\"\\n\\nIs not the same as \\n\\n\"\"The term Historical Jesus refers to scholarly reconstructions of portraits of the life of Jesus of Nazareth.[3][4][5]\"\"\\n\\nTo go out and dramatically change what the sources are saying is probably a gross misrepresentation of those sources.   \"   \n",
       "5828                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \"\\nIt should not be deleted, but fixed. North Kosovo is de facto independent from the Albanian-dominated government in Pristina, while other enclaves south are not. speaks \"   \n",
       "87827                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               I'm back... \\n\\n...I haven't found the rusty knife yet, but I'm working on it. Now, I believe we have matters to discuss. Like wtf do you think you are playing at?!?! 217.41.238.156   \n",
       "\n",
       "        toxic  \n",
       "145956      0  \n",
       "41086       0  \n",
       "127657      0  \n",
       "5828        0  \n",
       "87827       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data[data.toxic == 1].sample(5, random_state=42))\n",
    "display(data[data.toxic == 0].sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af2c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e41cfca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.toxic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c232bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd01f65",
   "metadata": {},
   "source": [
    "Данные загружены и осмотрены. Пропусков и явных дубликатов нет, Целевой признак сильно разбалансирован.\n",
    "Изучим длину слов в сообщениях, поскольку наличие сверхдлинных слов может помешать дальнейшей обработке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c406009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dad0507a888411ab198acf9de71f9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=19912), Label(value='0 / 19912')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\monah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\monah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"C:\\Users\\monah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandarallel\\core.py\", line 158, in __call__\n    results = self.work_function(\n  File \"C:\\Users\\monah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandarallel\\data_types\\series.py\", line 26, in work\n    return data.apply(\n  File \"C:\\Users\\monah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py\", line 4433, in apply\n    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n  File \"C:\\Users\\monah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py\", line 1082, in apply\n    return self.apply_standard()\n  File \"C:\\Users\\monah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py\", line 1137, in apply_standard\n    mapped = lib.map_infer(\n  File \"pandas\\_libs\\lib.pyx\", line 2870, in pandas._libs.lib.map_infer\n  File \"C:\\Users\\monah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandarallel\\progress_bars.py\", line 206, in closure\n    state.next_put_iteration += max(int((delta_i / delta_t) * 0.25), 1)\nZeroDivisionError: float division by zero\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest_word\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_word_length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mlongest_word\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen()\n\u001b[0;32m      3\u001b[0m data\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_word_length\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandarallel\\core.py:433\u001b[0m, in \u001b[0;36mparallelize_with_pipe.<locals>.closure\u001b[1;34m(data, user_defined_function, *user_defined_function_args, **user_defined_function_kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m worker_status \u001b[38;5;241m==\u001b[39m WorkerStatus\u001b[38;5;241m.\u001b[39mError:\n\u001b[0;32m    431\u001b[0m         progress_bars\u001b[38;5;241m.\u001b[39mset_error(worker_index)\n\u001b[1;32m--> 433\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mresults_promise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_type\u001b[38;5;241m.\u001b[39mreduce(results, reduce_extra)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "data[ 'longest_word'] = data.text.parallel_apply(get_max_length)\n",
    "data['max_word_length'] = data.longest_word.str.len()\n",
    "data.sort_values(by='max_word_length', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22fbe6",
   "metadata": {},
   "source": [
    "А также длину сообщений и количество слов в них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length'] = data.text.str.len()\n",
    "data.length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data[data.toxic == 1]['length'].describe())\n",
    "display(data[data.toxic == 0]['length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['n_words'] = data.text.apply(lambda x: len(x.split()))\n",
    "data.n_words.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70ee9c",
   "metadata": {},
   "source": [
    "Сверхдлинные слова чаще всего сочетаются с токсичностью, среднее количество слов в сообщениях - около семидесяти, но есть и гораздо более длинные сообщения.\n",
    "Общая длина сообщений по количеству символов ограничена пятью тысячами.\n",
    "Далее мы создадим паттерн с некоторыми грубыми лексемами, создадим признаки на его основе и изучим соответствие наличия грубости токсичности сообщения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96923c8",
   "metadata": {},
   "source": [
    "## Паттерн грубости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607657fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rudeness_pattern = re.compile(r'fuck|fuk|bitch|cunt|basta|puke|vomit|boner|sod off|bugger|tosser| cock|nigger|nigga|hate you|piss off|shut up|suck|shit|damn|dick|twat|queer|faggot|jerk|buttocs|dumb|bloody')\n",
    "data['rude'] = data.text.parallel_apply(find_pattern, pattern=rudeness_pattern)\n",
    "data['rudeness'] = data.rude.progress_apply(lambda x: len(x) > 0)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data[data.rudeness & (data.toxic == False)].head(10))\n",
    "display(data[~data.rudeness & (data.toxic == True)].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[data.rudeness & data.toxic == 1].shape[0])\n",
    "print(data[~data.rudeness & data.toxic == 1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cedfd1",
   "metadata": {},
   "source": [
    "В половине случаев токсичность соответствует наличию обсценной лексики, посмотрим, каково будет качество предсказаний по метрике f1, если прямолинейно  использовать наличие грубости в сообщениях, как показатель их токсичности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ecf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(data.toxic, data.rudeness.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33528a5a",
   "metadata": {},
   "source": [
    "Приравнивание наличие грубости токсичности сообщения дает хорошее отправное значение для дальнейших мероприятий."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182bd1dc",
   "metadata": {},
   "source": [
    " Наконец посмотрим, как соответствует количество грубых сообщений максимальной длине слов в них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79330152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Признак грубости и длина слов  \n",
    "step = 10\n",
    "for length in np.arange(data.max_word_length.min(), data.max_word_length.max(), step):\n",
    "    print(f'{length} - {length + step}:', data[(data.max_word_length >= length) & (data.max_word_length < length + step)]['rudeness'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c625d8",
   "metadata": {},
   "source": [
    "Действительно, хотя подавляющее большинство грубости не соответствует наличию сверхдлинных слов, но таковые встречаются и могут маскировать грубые выражения.\n",
    "Таким образом, на этапе изучения были сделаны следующие выводы и предположения:\n",
    "- Данные, в целом, готовы для дальнейшей работы, но\n",
    "    - необходимо учесть наличие очень длинных слов, не являющихся URL-адресами, что может значительно увеличить время  обработки текста, вызвать исключения и снизить в известной степени качество предсказаний за счет маскировки обсценной лексики\n",
    "    - необходимо также учесть наличие некоторого количества длинных сообщений\n",
    "- Выделение и дальнейшее использование признака грубости гипотетически может улучшить качество работы модели, поскольку в половине случаев этот признак соответствует токсичности\n",
    "\n",
    "Ниже мы подготовим основной текстовый признак, наборы данных и перейдем к работе с векторизацией и моделями"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3b12b",
   "metadata": {},
   "source": [
    "## Предобработка\n",
    "\n",
    "В ходе серии экспериментов была подобрана оптимальная схема предобработки текстового признака, инкапсулированная в функции preprocess().\n",
    "Эта схема включает в себя:\n",
    "- очистку от неалфавитных символов\n",
    "- фильтрацию по длине слов и количеству слов в сообщении (более длинные слова выбрасываются, более длинные сообщения обрезаются)\n",
    "- фильтрацию стоп-слов\n",
    "- стеммизацию\n",
    "- обратную сборку текста\n",
    "\n",
    "Эксперименты показали, что стеммизация дает лучшие результаты, чем лемматизация, отсутствие приведения к одному регистру никак не проявляется,а фильтрация  значительно улучшает скорость работы.\n",
    "Фильтрация же URL-адресов негативно сказывается на качестве.\n",
    "\n",
    "Далее мы обогатим полученный признак за счет признака грубости, разделим выборку на обучающую и тестовую в соотношении 3 : 1с учетом дисбаланса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stems'] = data.text.parallel_apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea26c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stems_rude'] = data.stems+ ' ' + data.rude\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.stems_rude, data.toxic, test_size=.1, stratify=data.toxic, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9aeeb",
   "metadata": {},
   "source": [
    "##  Создание конвейера автоматизированной обработки\n",
    "\n",
    "Конвейер состоит из TF-IDF-векторизатора и Ridge-классификатора, гиперпараметры которых подбирались вручную на небольшом сэмпле данных.\n",
    "Вначале посмотрим, какой результат получается при использовании подготовленного и обогащенного текстового признака, затем, чтобы убедиться в целессообразности примененной в отношении табуированной лексики манипуляции проведем проверку с использованием чистого подготовленного признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db58310",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    lowercase=False,\n",
    "    max_features=None,\n",
    "    max_df=0.3,\n",
    "    sublinear_tf=True,\n",
    "    smooth_idf=False,\n",
    ")\n",
    "classifier = RidgeClassifier(alpha=1.2, random_state=42, class_weight=\"balanced\")\n",
    "pipe = Pipeline(steps=[(\"vectorizer\", vectorizer), (\"classifier\", classifier)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(pipe, X_train, y_train, cv=3,\n",
    "               scoring='f1', n_jobs=-1)['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stems, _, y_train_stems, _ = train_test_split(data.stems, data.toxic, random_state=42, stratify=data.toxic)\n",
    "cross_validate(pipe, X_train_stems, y_train_stems, scoring='f1', n_jobs=-1, cv=3)['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153978bd",
   "metadata": {},
   "source": [
    "Примененная техника дает прирост качества в пару процентов, основная же эффективность достигается за счет настройки векторизатора. Гиперпараметры классификатора, за исключением alpha, остались на заводских значениях."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5368bf",
   "metadata": {},
   "source": [
    "## Апробация других моделей\n",
    "\n",
    "Здесь мы попробуем применить модели на основе машины опорных векторов, градиентного спуска, к-ближайших соседей и случайного леса. Заранее скажем, что все они, особенно первая, гораздо более требовательны к временным ресурсам и дают (в особенности третья) гораздо худший результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4562e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(classifier=SVC(random_state=42))\n",
    "cross_validate(pipe, X_train, y_train, scoring='f1', n_jobs=-1, cv=3)['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cad361",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(classifier=SGDClassifier(random_state=42, n_jobs=-1, early_stopping=True))\n",
    "cross_validate(pipe, X_train, y_train, scoring='f1', n_jobs=-1, cv=3)['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(classifier=KNeighborsClassifier(n_jobs=-1))\n",
    "cross_validate(pipe, X_train, y_train, scoring='f1', cv=3, n_jobs=-1)['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93895c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(classifier=RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "cross_validate(pipe, X_train, y_train, scoring='f1', n_jobs=-1, cv=3)['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456b042",
   "metadata": {},
   "source": [
    "Таким образом, лучшие показатели, как по требуемым ресурсам, так и по качеству, у модели на основе гребневой регрессии. Именно ее в сочетании с TF-IDF-векторизатором мы и рекомендуем для дальнейшего использования.\n",
    "Чтобы убедиться в адекватности ее работы, проверим предсказания на тестовой выборке, обучив модель на тренировочной."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312c5be",
   "metadata": {},
   "source": [
    "## Финальное тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18efd1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(classifier = RidgeClassifier(alpha=1.2, random_state=42, class_weight='balanced'))\n",
    "pipe.fit(X_train, y_train)\n",
    "predictions = pipe.predict(X_test)\n",
    "f1_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5dd76",
   "metadata": {},
   "source": [
    "Даже лучше, чем средний показатель при кросс-валидации на обучающей выборке, что свидетельствует о полном отсутствии переобучения.\n",
    "Взглянем на более подробный отчет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8184cac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(classification_report(y_test, predictions, output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16ddb4",
   "metadata": {},
   "source": [
    "Есть некоторые сложности в правильном предсказании метки 1, что вполне ожидаемо."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78004c82",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "В ходе настоящей работы были изучены и обработаны данные в виде набора текстовых сообщений с целью подобрать максимально эффективный механизм определения их токсичности.\n",
    "Для увеличения эффективности предсказания была применена техника выделения обсценной лексики для усиления сообветствующего текста.\n",
    "Текстовый признак был обработан с помощью фильтрации по характеру символов, длине и количеству слов и  стеммизации.\n",
    "Использовалась TF-IDF-векторизация, был проведен ручной поиск ее гиперпараметров,  был проведен поиск среди нескольких моделей машинного обубчения, в результате которого выбор был сделан в пользу классификации на основе гребневой регрессии.\n",
    "        Было проведено финальное тестирование, подтвердившее работоспособность выбранной схемы.Цели и задачи работы полностью реализованы, исследование успешно закончено."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
